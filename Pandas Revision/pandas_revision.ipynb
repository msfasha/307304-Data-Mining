{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lecture: 9 Must‑Know Pandas Operations for Working with Data\n\nThis notebook is a step‑by‑step lecture. Each concept includes:\n1) a short explanation,\n2) a concrete example, and\n3) what to look for in the output.\n\nAll examples are self‑contained and use small, accessible datasets. Run cells in order.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup\n\nWe import pandas and numpy, and set display options so tables are easier to read.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", 120)\npd.__version__\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Data Import\n\nWe demonstrate common ways to load data: CSV, Excel (via a generated file), JSON, SQL-like (using sqlite), and Parquet.\nThe examples ensure you can run them offline by creating small files on the fly.\n\nLook for: each load method returns a DataFrame with the expected columns and rows.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# CSV: create and read\ndf_csv_src = pd.DataFrame({'id':[1,2,3], 'name':['Ada','Bob','Cai'], 'score':[88,91,79]})\ncsv_path = 'example.csv'\ndf_csv_src.to_csv(csv_path, index=False)\ndf_csv = pd.read_csv(csv_path)\ndf_csv\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Excel: create and read from a specific sheet\nxlsx_path = 'example.xlsx'\nwith pd.ExcelWriter(xlsx_path) as writer:\n    df_csv_src.to_excel(writer, index=False, sheet_name='Sheet1')\n    (df_csv_src.assign(score=lambda d: d['score']+1)).to_excel(writer, index=False, sheet_name='Sheet2')\n\ndf_xlsx = pd.read_excel(xlsx_path, sheet_name='Sheet1')\ndf_xlsx\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# JSON: read from a JSON string/file\ndata_json = [\n    {\"id\": 1, \"dept\": \"A\", \"active\": True},\n    {\"id\": 2, \"dept\": \"B\", \"active\": False}\n]\njson_path = 'example.json'\npd.Series(data_json).to_json(json_path, orient='values')\ndf_json = pd.read_json(json_path)\ndf_json\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# SQL: use sqlite to simulate read_sql\nimport sqlite3\nconn = sqlite3.connect(':memory:')\npd.DataFrame({'x':[10,20,30],'y':['p','q','r']}).to_sql('tbl', conn, index=False, if_exists='replace')\ndf_sql = pd.read_sql('select * from tbl where x >= 20', conn)\ndf_sql\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Parquet: write and read a parquet file (needs pyarrow or fastparquet; we fall back to CSV if unavailable)\nparquet_path = 'example.parquet'\ntry:\n    df_csv_src.to_parquet(parquet_path)\n    df_parquet = pd.read_parquet(parquet_path)\nexcept Exception as e:\n    df_parquet = pd.read_csv(csv_path)  # fallback for environments without parquet engine\ndf_parquet\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Selection\n\nWe practice selecting columns and rows using `[]`, `.loc` (label-based), `.iloc` (integer-based), boolean filters,\n`query`, and membership with `isin`.\n\nExample dataset: a small customer table.\nLook for: how each selection returns either a Series or a DataFrame with the intended subset.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "customers = pd.DataFrame({\n    'customer_id':[101,102,103,104,105],\n    'name':['Alice','Bruno','Chin','Diana','Evan'],\n    'age':[25,41,33,29,52],\n    'city':['NY','SF','LA','NY','LA'],\n    'spend':[120.5, 340.0, 85.2, 220.0, 560.0]\n})\ncustomers\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Single column (Series)\ncustomers['age'].head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Label-based selection with .loc: choose rows by labels and columns by names\ncustomers.loc[1:3, ['name','city','spend']]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Integer-based selection with .iloc: choose rows and columns by integer positions\ncustomers.iloc[0:3, 0:3]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Boolean filtering\ncustomers[customers['spend'] > 200][['name','spend']]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# SQL-like filtering with .query\ncustomers.query('age >= 30 and city == \"LA\"')\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Multiple values with .isin\ncustomers[customers['city'].isin(['NY','LA'])]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Data Manipulation\n\nWe demonstrate grouping, merging, pivot tables, sorting, melting (unpivot), and applying functions.\n\nExample: two sales tables for merging and a line-item table for pivot/melt.\nLook for: how groupby aggregates per key, how merges combine columns, and how pivot/melt reshape the data.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "orders = pd.DataFrame({\n    'order_id':[1,2,3,4,5],\n    'customer_id':[101,101,102,104,105],\n    'amount':[120,80,340,220,560],\n    'channel':['web','store','web','web','store']\n})\ncustomers_small = customers[['customer_id','name','city']]\norders, customers_small.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Groupby with multiple aggregations\norders.groupby('customer_id').agg(total_amount=('amount','sum'),\n                                  avg_amount=('amount','mean'),\n                                  n_orders=('order_id','count')).reset_index()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Merge: left-join orders with customers to enrich records\norders_enriched = orders.merge(customers_small, on='customer_id', how='left')\norders_enriched\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Pivot table: sales by city and channel\npivot = pd.pivot_table(orders_enriched, values='amount', index='city', columns='channel', aggfunc='sum', fill_value=0)\npivot\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Sort values by amount then by channel\norders.sort_values(['amount','channel'], ascending=[False, True])\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Melt (unpivot): turn wide pivot back to long format\npivot_reset = pivot.reset_index()\nmelted = pivot_reset.melt(id_vars='city', value_vars=pivot.columns.tolist(), var_name='channel', value_name='amount')\nmelted.sort_values(['city','channel']).head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Apply: vectorized transformation of a column\norders.assign(amount_with_tax=lambda d: d['amount'] * 1.07)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Data Cleaning\n\nWe cover dropping missing values, forward-filling, removing duplicates, replacing values, casting types, and interpolation.\n\nExample dataset includes intentional issues: missing values, duplicates, and inconsistent coding.\nLook for: post-cleaning outputs have no duplicates, reasonable types, and filled values where appropriate.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "raw = pd.DataFrame({\n    'id':[1,2,2,3,4,5],\n    'category':['old','new','new','old',None,'old'],\n    'value':[10,np.nan,20,15,30,np.nan]\n})\nraw\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Drop rows with any missing in selected columns\ndropped = raw.dropna(subset=['category'], how='any')\ndropped\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Forward fill missing values in 'value'\nffill = raw.sort_values('id').copy()\nffill['value'] = ffill['value'].fillna(method='ffill')\nffill\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Remove duplicated ids (keep first)\nno_dupes = raw.drop_duplicates(subset=['id'])\nno_dupes\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Replace values\nreplaced = raw.copy()\nreplaced['category'] = replaced['category'].replace({'old':'legacy'})\nreplaced\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cast types\ntyped = raw.copy()\ntyped['id'] = typed['id'].astype('int64')\ntyped.dtypes\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Interpolate numeric series linearly\ninterpolated = raw.copy()\ninterpolated['value'] = interpolated['value'].interpolate(method='linear')\ninterpolated\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. String Operations\n\nWe demonstrate `str.contains`, `str.extract`, `str.split`, `str.lower`, `str.strip`, and `str.replace`.\n\nExample: product codes encoded in text.\nLook for: how vectorized `.str` methods operate on entire columns.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "products = pd.DataFrame({\n    'sku':['A-100-NY','B-250-SF','C-010-LA','B-333-NY','A-120-LA'],\n    'desc':['Widget Alpha','Bolt Pack 250','Cable 10ft','Bolt Pack 333','Widget Alpha Pro']\n})\nproducts\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# str.contains: find rows describing Bolt\nproducts[products['desc'].str.contains('Bolt', case=False)]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# str.extract: capture the numeric part in sku\nproducts['qty'] = products['sku'].str.extract(r'-(\\d+)-')\nproducts\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# str.split: split into parts and select the city part\nproducts['city'] = products['sku'].str.split('-').str[2]\nproducts[['sku','city']]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# str.lower and str.strip\nproducts['norm_desc'] = products['desc'].str.lower().str.strip()\nproducts[['desc','norm_desc']]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# str.replace: remove non-alphanumerics from desc\nproducts['desc_simple'] = products['desc'].str.replace(r'[^0-9a-zA-Z ]+','', regex=True)\nproducts[['desc','desc_simple']]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Statistics\n\nWe compute summary statistics, per-column aggregations, value counts, correlations, covariance, and quantiles.\n\nExample: reuse the `orders_enriched` table.\nLook for: distribution summaries and relationships between numeric variables.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Summary statistics\norders_enriched[['amount']].describe()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Per-column aggregation with agg on a column\norders_enriched['amount'].agg(['mean','median','std'])\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Value counts (normalized share of orders by channel)\norders_enriched['channel'].value_counts(normalize=True)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Correlation matrix among numeric columns\norders_enriched[['amount','customer_id']].corr(method='pearson')\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Covariance matrix\norders_enriched[['amount','customer_id']].cov()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Quantiles\norders_enriched['amount'].quantile([0.25, 0.5, 0.75])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Time Series\n\nWe construct a realistic daily sales time series with seasonality and noise to demonstrate resampling, rolling windows,\nshifts, date_range creation, frequency conversion with `asfreq`, and datetime formatting with `dt.strftime`.\n\nLook for: monthly resampling aggregates, moving averages smoothing fluctuations, and shifted series aligning past values.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "rng = pd.date_range('2024-01-01', periods=365, freq='D')\nrng.name = 'date'\nnp.random.seed(7)\n\nbase = 200 + 20*np.sin(2*np.pi*(rng.dayofyear)/7)  # weekly seasonality\ntrend = np.linspace(0, 50, len(rng))               # upward trend across the year\nnoise = np.random.normal(0, 10, len(rng))\nsales = np.maximum(0, base + trend + noise)\n\nts = pd.DataFrame({'sales': sales}, index=rng)\nts.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Resample to monthly average sales\nmonthly_avg = ts['sales'].resample('M').mean()\nmonthly_avg.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Rolling 7-day mean to smooth noise\nts['rolling_7d'] = ts['sales'].rolling(window=7).mean()\nts[['sales','rolling_7d']].head(12)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Shift the series by one period to compare to \"yesterday\"\nts['sales_lag1'] = ts['sales'].shift(periods=1)\nts[['sales','sales_lag1']].head(5)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Create a custom date_range: first business day of each month in 2024\ncustom_range = pd.date_range('2024-01-01', periods=12, freq='BMS')\ncustom_range\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# asfreq to insert explicit daily frequency with forward fill of missing (illustration by dropping dates then restoring)\nts_thin = ts.iloc[::3].copy()         # keep every 3rd day\nts_ffill = ts_thin.asfreq('D').ffill()\nts_ffill.head(10)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Datetime formatting with dt.strftime\ndate_labels = ts.index.to_series().dt.strftime('%Y-%m-%d')\ndate_labels.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Advanced Features\n\nWe show `pipe` for method chaining, `eval` for expression evaluation, `memory_usage`, `select_dtypes`, `nlargest`,\nand `explode` for list-like columns.\n\nLook for: cleaner pipelines with `pipe`, selections by dtype, and expansion of list columns with `explode`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# pipe: define a function and insert into a chain\ndef add_net_amount(df, tax_rate=0.07):\n    return df.assign(net_amount=lambda d: d['amount']*(1+tax_rate))\n\nchained = (orders_enriched\n           .pipe(add_net_amount, tax_rate=0.05)\n           .sort_values('net_amount', ascending=False)\n           .head(3))\nchained\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# eval: compute an expression over columns\norders_enriched.eval('double_amount = amount * 2', inplace=False)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# memory_usage: inspect approximate memory footprint\norders_enriched.memory_usage(deep=True).sum()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# select_dtypes: pick numeric columns only\norders_enriched.select_dtypes(include=['number']).head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# nlargest: top-N orders by amount\norders_enriched.nlargest(3, 'amount')\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# explode: expand list-like column to long rows\nmulti = pd.DataFrame({\n    'order_id':[10,11],\n    'items':[ ['A','B','C'], ['B'] ]\n})\nmulti_exploded = multi.explode('items', ignore_index=True)\nmulti_exploded\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Data Export\n\nWe demonstrate exporting to CSV, Excel, JSON, and Parquet. Files are written to the current folder.\nOpen them after running if you'd like to inspect contents.\n\nLook for: files created with the expected shape and fields.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "orders_enriched.to_csv('orders_out.csv', index=False)\nwith pd.ExcelWriter('orders_out.xlsx') as w:\n    orders_enriched.to_excel(w, index=False, sheet_name='Sheet1')\norders_enriched.to_json('orders_out.json', orient='records')\n\ntry:\n    orders_enriched.to_parquet('orders_out.parquet')\n    export_status = 'CSV, Excel, JSON, Parquet exported.'\nexcept Exception:\n    export_status = 'CSV, Excel, JSON exported. Parquet skipped (engine unavailable).'\nexport_status\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Tips and Best Practices\n\n1. Use `.copy()` when creating DataFrame views that you plan to modify to avoid chained assignment issues.\n2. Prefer method chaining with `pipe` and temporary variables kept to a minimum for readability.\n3. For categorical data, set `dtype='category'` to save memory and enable category-aware operations.\n4. Use `inplace=True` sparingly. It often returns `None` and can make pipelines harder to reason about.\n5. Validate assumptions early with `df.sample()`, `df.info()`, and `.describe()` before heavy transformations.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}