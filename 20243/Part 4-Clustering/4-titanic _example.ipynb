{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78650b3",
   "metadata": {},
   "source": [
    "# Titanic Dataset K-means Clustering Analysis\n",
    "\n",
    "This comprehensive notebook demonstrates how to perform K-means clustering on the Titanic dataset to discover hidden patterns and group passengers based on their characteristics. We'll explore every step of the machine learning pipeline from data preprocessing to actionable insights.\n",
    "\n",
    "**What is K-means Clustering?**\n",
    "K-means is an unsupervised machine learning algorithm that groups data points into k clusters based on feature similarity. It works by finding cluster centers (centroids) and assigning each data point to the nearest centroid.\n",
    "\n",
    "**Why use clustering on Titanic data?**\n",
    "- Discover passenger segments with similar characteristics\n",
    "- Understand survival patterns across different groups\n",
    "- Identify risk factors and safety insights\n",
    "- Segment passengers for targeted analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Required Libraries\n",
    "\n",
    "Before we start our analysis, we need to import essential Python libraries. Each library serves a specific purpose in our machine learning pipeline.\n",
    "\n",
    "**Libraries explanation:**\n",
    "- **pandas**: Data manipulation and analysis (think Excel but more powerful)\n",
    "- **numpy**: Numerical computations and array operations\n",
    "- **matplotlib & seaborn**: Data visualization and plotting\n",
    "- **sklearn**: Machine learning algorithms and tools\n",
    "- **warnings**: Suppress unnecessary warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd  # For dataframes, data loading, and manipulation\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt  # Basic plotting functionality\n",
    "import seaborn as sns           # Statistical data visualization (prettier plots)\n",
    "\n",
    "# Machine learning components\n",
    "from sklearn.cluster import KMeans              # K-means clustering algorithm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Data preprocessing tools\n",
    "from sklearn.decomposition import PCA          # Principal Component Analysis for dimensionality reduction\n",
    "from sklearn.metrics import silhouette_score   # Clustering quality measurement\n",
    "\n",
    "# Utility imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Hide warning messages for cleaner output\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09fe33",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Data exploration is the foundation of any successful machine learning project. We need to understand our data structure, types, missing values, and basic statistics before proceeding.\n",
    "\n",
    "**Key concepts:**\n",
    "- **Dataset shape**: Number of rows (samples) and columns (features)\n",
    "- **Data types**: Numerical vs categorical variables\n",
    "- **Missing values**: Gaps in data that need handling\n",
    "- **Statistical summary**: Mean, median, standard deviation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27b651",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "# Method 1: If you have a CSV file uploaded to Colab\n",
    "# df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Method 2: Use seaborn's built-in dataset (recommended for this tutorial)\n",
    "df = sns.load_dataset('titanic')  # Loads the famous Titanic dataset\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"Dataset shape:\", df.shape)  # (rows, columns) - tells us dataset size\n",
    "print(\"\\nColumn names and types:\")\n",
    "print(df.dtypes)  # Shows data type of each column (int64, float64, object, etc.)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())  # Display first 5 rows to see data structure\n",
    "\n",
    "print(\"\\nDataset statistical summary:\")\n",
    "print(df.describe())  # Statistical summary for numerical columns\n",
    "\n",
    "print(\"\\nDetailed dataset information:\")\n",
    "print(df.info())  # Memory usage, non-null counts, data types\n",
    "\n",
    "print(\"\\nMissing values count:\")\n",
    "missing_values = df.isnull().sum()  # Count missing values in each column\n",
    "print(missing_values[missing_values > 0])  # Show only columns with missing values\n",
    "\n",
    "print(\"\\nUnique values in categorical columns:\")\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values -> {list(df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e6057",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc0e3c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ecbae9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing is crucial for clustering success. Raw data often contains missing values, inconsistent formats, and needs transformation for machine learning algorithms.\n",
    "\n",
    "**Key preprocessing steps:**\n",
    "1. **Missing value imputation**: Fill gaps in data with appropriate values\n",
    "2. **Feature engineering**: Create new meaningful features from existing ones\n",
    "3. **Categorical encoding**: Convert text categories to numbers\n",
    "4. **Feature scaling**: Normalize numerical ranges\n",
    "\n",
    "**Why preprocessing matters:**\n",
    "- K-means uses distance calculations, so all features need similar scales\n",
    "- Missing values can break algorithms\n",
    "- Well-engineered features improve clustering quality\n",
    "\n",
    "```python\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing function for Titanic dataset.\n",
    "    \n",
    "    This function handles:\n",
    "    - Missing value imputation using statistical methods\n",
    "    - Feature engineering to create meaningful new variables\n",
    "    - Categorical variable encoding for machine learning compatibility\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Raw Titanic dataset\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Preprocessed dataset ready for clustering\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataset\n",
    "    data = df.copy()\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # === MISSING VALUE IMPUTATION ===\n",
    "    print(\"\\n1. Handling missing values:\")\n",
    "    \n",
    "    # Age: Use median (robust to outliers, represents typical passenger age)\n",
    "    median_age = data['age'].median()\n",
    "    data['age'] = data['age'].fillna(median_age)\n",
    "    print(f\"   ‚Ä¢ Filled {df['age'].isnull().sum()} missing ages with median: {median_age:.1f}\")\n",
    "    \n",
    "    # Fare: Use median (some passengers had free tickets or unknown fares)\n",
    "    median_fare = data['fare'].median()\n",
    "    data['fare'] = data['fare'].fillna(median_fare)\n",
    "    print(f\"   ‚Ä¢ Filled {df['fare'].isnull().sum()} missing fares with median: ${median_fare:.2f}\")\n",
    "    \n",
    "    # Embarked: Use mode (most common port)\n",
    "    mode_embarked = data['embarked'].mode()[0]  # mode() returns a Series, [0] gets the value\n",
    "    data['embarked'] = data['embarked'].fillna(mode_embarked)\n",
    "    print(f\"   ‚Ä¢ Filled {df['embarked'].isnull().sum()} missing embarkation ports with mode: {mode_embarked}\")\n",
    "    \n",
    "    # Deck: Fill with 'Unknown' (many passengers had unknown deck assignments)\n",
    "    deck_missing = data['deck'].isnull().sum()\n",
    "    data['deck'] = data['deck'].fillna('Unknown')\n",
    "    print(f\"   ‚Ä¢ Filled {deck_missing} missing deck assignments with 'Unknown'\")\n",
    "    \n",
    "    # === FEATURE ENGINEERING ===\n",
    "    print(\"\\n2. Creating new features:\")\n",
    "    \n",
    "    # Family size: Total family members aboard (including passenger)\n",
    "    data['family_size'] = data['sibsp'] + data['parch'] + 1\n",
    "    print(f\"   ‚Ä¢ Created 'family_size': sibsp + parch + 1 (range: {data['family_size'].min()}-{data['family_size'].max()})\")\n",
    "    \n",
    "    # Is alone: Boolean indicator for solo travelers\n",
    "    data['is_alone'] = (data['family_size'] == 1).astype(int)  # Convert boolean to 0/1\n",
    "    alone_count = data['is_alone'].sum()\n",
    "    print(f\"   ‚Ä¢ Created 'is_alone': {alone_count} passengers ({alone_count/len(data)*100:.1f}%) traveled alone\")\n",
    "    \n",
    "    # Age groups: Categorize ages into life stages\n",
    "    data['age_group'] = pd.cut(data['age'], \n",
    "                              bins=[0, 18, 35, 55, 100],  # Age boundaries\n",
    "                              labels=['Child', 'Young', 'Middle', 'Senior'],  # Category names\n",
    "                              include_lowest=True)  # Include boundary values\n",
    "    print(f\"   ‚Ä¢ Created 'age_group': {data['age_group'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Fare groups: Economic status indicator based on ticket price\n",
    "    data['fare_group'] = pd.cut(data['fare'], \n",
    "                               bins=[0, 10, 50, 100, 1000],  # Price boundaries\n",
    "                               labels=['Low', 'Medium', 'High', 'Very High'],  # Economic levels\n",
    "                               include_lowest=True)\n",
    "    print(f\"   ‚Ä¢ Created 'fare_group': {data['fare_group'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # === CATEGORICAL ENCODING ===\n",
    "    print(\"\\n3. Encoding categorical variables:\")\n",
    "    \n",
    "    # LabelEncoder converts categories to numbers (0, 1, 2, ...)\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = ['sex', 'embarked', 'class', 'who', 'deck', 'age_group', 'fare_group']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in data.columns:\n",
    "            # Create new encoded column (keeps original for reference)\n",
    "            encoded_col = col + '_encoded'\n",
    "            data[encoded_col] = le.fit_transform(data[col].astype(str))\n",
    "            \n",
    "            # Show encoding mapping\n",
    "            unique_values = data[col].unique()\n",
    "            encoded_values = le.fit_transform(unique_values.astype(str))\n",
    "            mapping = dict(zip(unique_values, encoded_values))\n",
    "            print(f\"   ‚Ä¢ Encoded '{col}': {mapping}\")\n",
    "    \n",
    "    print(f\"\\nPreprocessing completed! Dataset shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to our dataset\n",
    "processed_df = preprocess_data(df)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\nPreprocessed dataset preview:\")\n",
    "print(processed_df.head()[['age', 'fare', 'family_size', 'is_alone', 'sex_encoded', 'embarked_encoded']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Selection and Preparation\n",
    "\n",
    "Feature selection is critical for clustering success. We need to choose features that are meaningful, non-redundant, and appropriate for distance-based algorithms.\n",
    "\n",
    "**Feature selection principles:**\n",
    "- **Relevance**: Features should relate to passenger characteristics\n",
    "- **Non-redundancy**: Avoid highly correlated features\n",
    "- **Measurability**: All features should be quantifiable\n",
    "- **Clustering-appropriate**: Suitable for distance calculations\n",
    "\n",
    "**Our feature categories:**\n",
    "1. **Demographics**: Age, gender\n",
    "2. **Socioeconomic**: Class, fare\n",
    "3. **Family structure**: Family size, traveling alone\n",
    "4. **Journey details**: Embarkation port, cabin class\n",
    "\n",
    "```python\n",
    "# Select features for clustering analysis\n",
    "print(\"=== FEATURE SELECTION FOR CLUSTERING ===\\n\")\n",
    "\n",
    "# Define our feature set with clear rationale\n",
    "feature_columns = [\n",
    "    'pclass',           # Passenger class (1st=1, 2nd=2, 3rd=3) - Socioeconomic status\n",
    "    'sex_encoded',      # Gender encoded (0=female, 1=male) - Demographic factor\n",
    "    'age',              # Age in years - Demographic factor\n",
    "    'sibsp',            # Siblings/spouses aboard - Family structure\n",
    "    'parch',            # Parents/children aboard - Family structure  \n",
    "    'fare',             # Ticket fare in pounds - Economic indicator\n",
    "    'embarked_encoded', # Port of embarkation encoded - Journey characteristic\n",
    "    'family_size',      # Total family size (engineered feature)\n",
    "    'is_alone'          # Traveling alone indicator (engineered feature)\n",
    "]\n",
    "\n",
    "print(\"Selected features and their business meaning:\")\n",
    "feature_descriptions = {\n",
    "    'pclass': 'Passenger class (1st, 2nd, 3rd) - Social/economic status',\n",
    "    'sex_encoded': 'Gender (0=female, 1=male) - Demographic characteristic',\n",
    "    'age': 'Age in years - Life stage and vulnerability',\n",
    "    'sibsp': 'Number of siblings/spouses - Family support system',\n",
    "    'parch': 'Number of parents/children - Family responsibilities',\n",
    "    'fare': 'Ticket fare - Economic capability and cabin quality',\n",
    "    'embarked_encoded': 'Embarkation port - Journey origin and class',\n",
    "    'family_size': 'Total family size - Social support network',\n",
    "    'is_alone': 'Solo traveler indicator - Social isolation factor'\n",
    "}\n",
    "\n",
    "for i, (feature, description) in enumerate(feature_descriptions.items(), 1):\n",
    "    print(f\"{i:2d}. {feature:<17} -> {description}\")\n",
    "\n",
    "# Create the feature matrix X\n",
    "X = processed_df[feature_columns].copy()\n",
    "print(f\"\\nFeature matrix created with shape: {X.shape}\")\n",
    "print(\"(Rows = passengers, Columns = features)\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(X.describe().round(2))\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values check:\")\n",
    "missing_check = X.isnull().sum()\n",
    "if missing_check.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Found missing values: {missing_check[missing_check > 0]}\")\n",
    "    # Handle any remaining missing values with median imputation\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"   Fixed with median imputation\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(\"Ready for clustering algorithm!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feature Standardization\n",
    "\n",
    "K-means clustering is sensitive to feature scales because it uses Euclidean distance. Features with larger scales (like fare in dollars) will dominate features with smaller scales (like number of siblings).\n",
    "\n",
    "**Why standardization is crucial:**\n",
    "- **Equal influence**: All features contribute equally to distance calculations\n",
    "- **Algorithm stability**: Prevents numerical issues and improves convergence\n",
    "- **Interpretability**: Standardized cluster centers are easier to interpret\n",
    "\n",
    "**Standardization process:**\n",
    "- Transform each feature to have mean = 0 and standard deviation = 1\n",
    "- Formula: (value - mean) / standard_deviation\n",
    "- Result: All features have similar scales\n",
    "\n",
    "```python\n",
    "print(\"=== FEATURE STANDARDIZATION ===\\n\")\n",
    "\n",
    "# Display original feature scales for comparison\n",
    "print(\"Original feature scales (before standardization):\")\n",
    "scale_comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Min': X.min().round(2),\n",
    "    'Max': X.max().round(2),\n",
    "    'Mean': X.mean().round(2),\n",
    "    'Std': X.std().round(2),\n",
    "    'Range': (X.max() - X.min()).round(2)\n",
    "})\n",
    "print(scale_comparison)\n",
    "\n",
    "print(f\"\\nProblem: Features have vastly different scales!\")\n",
    "print(f\"‚Ä¢ Fare ranges from ${X['fare'].min():.0f} to ${X['fare'].max():.0f} (range: {X['fare'].max() - X['fare'].min():.0f})\")\n",
    "print(f\"‚Ä¢ Age ranges from {X['age'].min():.0f} to {X['age'].max():.0f} years (range: {X['age'].max() - X['age'].min():.0f})\")\n",
    "print(f\"‚Ä¢ Sex encoded is only 0 or 1 (range: {X['sex_encoded'].max() - X['sex_encoded'].min():.0f})\")\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "print(f\"\\nApplying StandardScaler transformation...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to our data and transform it\n",
    "# fit() calculates mean and std for each feature\n",
    "# transform() applies the standardization formula\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"StandardScaler process:\")\n",
    "print(\"1. fit() - Calculate mean and standard deviation for each feature\")\n",
    "print(\"2. transform() - Apply formula: (value - mean) / std\")\n",
    "\n",
    "# Convert back to DataFrame for easier analysis\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Verify standardization worked correctly\n",
    "print(f\"\\nAfter standardization verification:\")\n",
    "print(\"‚úÖ All features should have mean ‚âà 0 and std ‚âà 1\")\n",
    "standardized_stats = pd.DataFrame({\n",
    "    'Feature': X_scaled_df.columns,\n",
    "    'Mean': X_scaled_df.mean().round(6),      # Should be ~0\n",
    "    'Std': X_scaled_df.std().round(6),        # Should be ~1\n",
    "    'Min': X_scaled_df.min().round(2),\n",
    "    'Max': X_scaled_df.max().round(2)\n",
    "})\n",
    "print(standardized_stats)\n",
    "\n",
    "# Show the transformation effect with examples\n",
    "print(f\"\\nStandardization example for 'fare' feature:\")\n",
    "original_fares = X['fare'].head(3).values\n",
    "standardized_fares = X_scaled_df['fare'].head(3).values\n",
    "for i in range(3):\n",
    "    print(f\"  Passenger {i+1}: ${original_fares[i]:.2f} ‚Üí {standardized_fares[i]:.3f}\")\n",
    "\n",
    "print(f\"\\nStandardized feature matrix ready!\")\n",
    "print(f\"Shape: {X_scaled.shape} (same as before, just transformed values)\")\n",
    "print(f\"Now all features contribute equally to distance calculations! üéØ\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Determine Optimal Number of Clusters\n",
    "\n",
    "Choosing the right number of clusters is crucial for meaningful results. We'll use two complementary methods to find the optimal k.\n",
    "\n",
    "**Methods for finding optimal k:**\n",
    "1. **Elbow Method**: Look for the \"elbow\" point where adding more clusters doesn't significantly reduce within-cluster variance\n",
    "2. **Silhouette Analysis**: Measures how well-separated clusters are (range: -1 to 1, higher is better)\n",
    "\n",
    "**Key concepts:**\n",
    "- **Inertia**: Sum of squared distances from points to their cluster centers (lower is better)\n",
    "- **Silhouette Score**: Average silhouette coefficient across all points (higher is better)\n",
    "- **Trade-off**: More clusters reduce inertia but may create overfitting\n",
    "\n",
    "```python\n",
    "def plot_elbow_method(X_scaled, max_k=10):\n",
    "    \"\"\"\n",
    "    Determine optimal number of clusters using Elbow Method and Silhouette Analysis.\n",
    "    \n",
    "    The Elbow Method plots inertia (within-cluster sum of squares) vs number of clusters.\n",
    "    The \"elbow\" point indicates optimal k where adding more clusters gives diminishing returns.\n",
    "    \n",
    "    Silhouette Score measures cluster separation quality:\n",
    "    - Score > 0.5: Good clustering\n",
    "    - Score > 0.7: Strong clustering  \n",
    "    - Score < 0.2: Poor clustering\n",
    "    \n",
    "    Args:\n",
    "        X_scaled (numpy.ndarray): Standardized feature matrix\n",
    "        max_k (int): Maximum number of clusters to test\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (K_range, inertias, silhouette_scores) for further analysis\n",
    "    \"\"\"\n",
    "    print(\"=== FINDING OPTIMAL NUMBER OF CLUSTERS ===\\n\")\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    inertias = []           # Within-cluster sum of squares\n",
    "    silhouette_scores = []  # Cluster separation quality\n",
    "    K_range = range(2, max_k + 1)  # Start from 2 (can't have 1 cluster for silhouette)\n",
    "    \n",
    "    print(\"Testing different numbers of clusters:\")\n",
    "    print(\"k | Inertia  | Silhouette | Interpretation\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Test each possible number of clusters\n",
    "    for k in K_range:\n",
    "        # Initialize K-means with specific parameters\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,      # Number of clusters to form\n",
    "            random_state=42,   # For reproducible results\n",
    "            n_init=10,         # Number of random initializations (chooses best)\n",
    "            max_iter=300       # Maximum iterations for convergence\n",
    "        )\n",
    "        \n",
    "        # Fit the model and predict cluster labels\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertia = kmeans.inertia_  # Within-cluster sum of squared distances\n",
    "        sil_score = silhouette_score(X_scaled, cluster_labels)  # Cluster quality\n",
    "        \n",
    "        # Store results\n",
    "        inertias.append(inertia)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Interpret silhouette score\n",
    "        if sil_score > 0.7:\n",
    "            interpretation = \"Excellent\"\n",
    "        elif sil_score > 0.5:\n",
    "            interpretation = \"Good\"\n",
    "        elif sil_score > 0.3:\n",
    "            interpretation = \"Fair\"\n",
    "        else:\n",
    "            interpretation = \"Poor\"\n",
    "            \n",
    "        print(f\"{k} | {inertia:8.2f} | {sil_score:10.3f} | {interpretation}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # === ELBOW METHOD PLOT ===\n",
    "    ax1.plot(K_range, inertias, 'bo-', markersize=8, linewidth=2, color='steelblue')\n",
    "    ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    ax1.set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
    "    ax1.set_title('Elbow Method for Optimal k\\n(Look for the \"elbow\" point)', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    for i, (k, inertia) in enumerate(zip(K_range, inertias)):\n",
    "        if i % 2 == 0:  # Annotate every other point to avoid crowding\n",
    "            ax1.annotate(f'k={k}\\n{inertia:.0f}', \n",
    "                        (k, inertia), \n",
    "                        textcoords=\"offset points\", \n",
    "                        xytext=(0,10), \n",
    "                        ha='center', fontsize=9)\n",
    "    \n",
    "    # === SILHOUETTE SCORE PLOT ===  \n",
    "    colors = ['red' if score < 0.3 else 'orange' if score < 0.5 else 'green' for score in silhouette_scores]\n",
    "    bars = ax2.bar(K_range, silhouette_scores, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "    ax2.set_title('Silhouette Score for Different k\\n(Higher is better, >0.5 is good)', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, silhouette_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Add color legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', alpha=0.7, label='Good (>0.5)'),\n",
    "        Patch(facecolor='orange', alpha=0.7, label='Fair (0.3-0.5)'),\n",
    "        Patch(facecolor='red', alpha=0.7, label='Poor (<0.3)')\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return K_range, inertias, silhouette_scores\n",
    "\n",
    "# Execute the analysis\n",
    "print(\"Analyzing cluster quality for k=2 to k=10...\")\n",
    "K_range, inertias, silhouette_scores = plot_elbow_method(X_scaled, max_k=10)\n",
    "\n",
    "# Provide recommendations\n",
    "print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "best_silhouette_k = K_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette_score = max(silhouette_scores)\n",
    "\n",
    "print(f\"üìä Best silhouette score: k={best_silhouette_k} (score: {best_silhouette_score:.3f})\")\n",
    "\n",
    "# Calculate elbow point (simplified method)\n",
    "# Look for the point with maximum rate of change decrease\n",
    "if len(inertias) > 2:\n",
    "    diffs = np.diff(inertias)  # First differences\n",
    "    diff_diffs = np.diff(diffs)  # Second differences (rate of change)\n",
    "    elbow_idx = np.argmax(diff_diffs) + 2  # +2 because of double differencing\n",
    "    elbow_k = K_range[elbow_idx] if elbow_idx < len(K_range) else best_silhouette_k\n",
    "    print(f\"üìà Elbow method suggests: k={elbow_k}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendation: Use k={best_silhouette_k} for best cluster separation\")\n",
    "print(f\"   This provides {best_silhouette_k} distinct passenger groups with good internal cohesion\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Choose Optimal K and Perform Clustering\n",
    "\n",
    "Now we'll implement the actual K-means clustering using our chosen optimal number of clusters.\n",
    "\n",
    "**K-means Algorithm Steps:**\n",
    "1. **Initialize**: Place k centroids randomly in feature space\n",
    "2. **Assign**: Assign each point to the nearest centroid\n",
    "3. **Update**: Move centroids to the center of their assigned points\n",
    "4. **Repeat**: Steps 2-3 until centroids stop moving (convergence)\n",
    "\n",
    "**Key parameters explained:**\n",
    "- **n_clusters**: Number of clusters (k)\n",
    "- **random_state**: Ensures reproducible results\n",
    "- **n_init**: Number of different random initializations (algorithm picks the best)\n",
    "- **max_iter**: Maximum iterations before stopping\n",
    "\n",
    "```python\n",
    "print(\"=== PERFORMING K-MEANS CLUSTERING ===\\n\")\n",
    "\n",
    "# Determine optimal k from our previous analysis\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Using optimal k = {optimal_k} (based on highest silhouette score)\")\n",
    "\n",
    "# Alternative: You can manually override if you prefer a different k\n",
    "# optimal_k = 3  # Uncomment this line to manually set k\n",
    "\n",
    "print(f\"\\nInitializing K-means algorithm with parameters:\")\n",
    "print(f\"‚Ä¢ n_clusters = {optimal_k} (number of clusters)\")\n",
    "print(f\"‚Ä¢ random_state = 42 (for reproducible results)\")\n",
    "print(f\"‚Ä¢ n_init = 10 (try 10 different random starts, pick best)\")\n",
    "print(f\"‚Ä¢ max_iter = 300 (maximum iterations before stopping)\")\n",
    "\n",
    "# Initialize and fit the K-means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=optimal_k,    # Our optimal number of clusters\n",
    "    random_state=42,         # Seed for reproducible results\n",
    "    n_init=10,              # Number of different centroid initializations\n",
    "    max_iter=300,           # Maximum number of iterations\n",
    "    tol=1e-4                # Tolerance for convergence (when to stop)\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting K-means model...\")\n",
    "print(\"Algorithm process:\")\n",
    "print(\"1. Randomly initialize cluster centroids\")\n",
    "print(\"2. Assign each passenger to nearest centroid\")\n",
    "print(\"3. Move centroids to center of assigned passengers\")\n",
    "print(\"4. Repeat steps 2-3 until centroids stabilize\")\n",
    "\n",
    "# Fit the model and get cluster predictions\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\n‚úÖ Clustering completed!\")\n",
    "print(f\"‚Ä¢ Converged in {kmeans.n_iter_} iterations\")\n",
    "print(f\"‚Ä¢ Final inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Add cluster labels to our original dataframe\n",
    "processed_df['cluster'] = cluster_labels\n",
    "\n",
    "# Calculate final clustering quality metrics\n",
    "final_silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f\"‚Ä¢ Final silhouette score: {final_silhouette:.3f}\")\n",
    "\n",
    "# Interpret silhouette score\n",
    "if final_silhouette > 0.7:\n",
    "    quality = \"Excellent clustering quality! üèÜ\"\n",
    "elif final_silhouette > 0.5:\n",
    "    quality = \"Good clustering quality! üëç\"\n",
    "elif final_silhouette > 0.3:\n",
    "    quality = \"Fair clustering quality üëå\"\n",
    "else:\n",
    "    quality = \"Poor clustering quality - consider different approach ‚ö†Ô∏è\"\n",
    "\n",
    "print(f\"‚Ä¢ Quality assessment: {quality}\")\n",
    "\n",
    "print(f\"\\nCluster centers (centroids) in standardized space:\")\n",
    "centroids_df = pd.DataFrame(\n",
    "    kmeans.cluster_centers_,\n",
    "    columns=feature_columns,\n",
    "    index=[f'Cluster {i}' for i in range(optimal_k)]\n",
    ")\n",
    "print(centroids_df.round(3))\n",
    "\n",
    "print(f\"\\nNote: These are standardized values (mean=0, std=1)\")\n",
    "print(f\"Positive values = above average, Negative values = below average\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Analyze Cluster Distribution and Basic Statistics\n",
    "\n",
    "Understanding the size and basic characteristics of each cluster helps us interpret what each group represents.\n",
    "\n",
    "**Key metrics to analyze:**\n",
    "- **Cluster sizes**: How many passengers in each group?\n",
    "- **Feature averages**: What are the typical characteristics of each cluster?\n",
    "- **Survival rates**: How did different clusters fare during the disaster?\n",
    "\n",
    "```python\n",
    "print(\"=== CLUSTER DISTRIBUTION ANALYSIS ===\\n\")\n",
    "\n",
    "# === CLUSTER SIZES ===\n",
    "print(\"1. CLUSTER SIZE DISTRIBUTION:\")\n",
    "cluster_distribution = processed_df['cluster'].value_counts().sort_index()\n",
    "total_passengers = len(processed_df)\n",
    "\n",
    "print(\"Cluster | Count | Percentage\")\n",
    "print(\"-\" * 30)\n",
    "for cluster_id, count in cluster_distribution.items():\n",
    "    percentage = (count / total_passengers) * 100\n",
    "    bar = \"‚ñà\" * int(percentage / 2)  # Visual bar representation\n",
    "    print(f\"{cluster_id:7d} | {count:5d} | {percentage:5.1f}% {bar}\")\n",
    "\n",
    "print(f\"\\nTotal passengers analyzed: {total_passengers}\")\n",
    "\n",
    "# Check for balanced clusters\n",
    "max_cluster_size = cluster_distribution.max()\n",
    "min_cluster_size = cluster_distribution.min()\n",
    "balance_ratio = max_cluster_size / min_cluster_size\n",
    "\n",
    "print(f\"Cluster balance ratio: {balance_ratio:.2f}\")\n",
    "if balance_ratio < 3:\n",
    "    print(\"‚úÖ Well-balanced clusters (no cluster dominates)\")\n",
    "elif balance_ratio < 5:\n",
    "    print(\"‚ö†Ô∏è  Somewhat imbalanced clusters\")\n",
    "else:\n",
    "    print(\"‚ùå Highly imbalanced clusters - consider different k\")\n",
    "\n",
    "# === DETAILED CLUSTER STATISTICS ===\n",
    "print(f\"\\n2. DETAILED CLUSTER STATISTICS:\")\n",
    "print(\"(Average values for each feature by cluster)\")\n",
    "\n",
    "# Calculate mean values for each cluster\n",
    "cluster_stats = processed_df.groupby('cluster')[feature_columns + ['survived']].agg(['mean', 'std']).round(3)\n",
    "\n",
    "# Simplify column names for better readability\n",
    "cluster_means = processed_df.groupby('cluster')[feature_columns + ['survived']].mean().round(3)\n",
    "\n",
    "print(\"\\nCluster feature averages:\")\n",
    "print(cluster_means)\n",
    "\n",
    "# === CLUSTER INTERPRETATION HELPER ===\n",
    "print(f\"\\n3. QUICK CLUSTER INTERPRETATION:\")\n",
    "print(\"(Comparing each cluster to overall averages)\")\n",
    "\n",
    "# Calculate overall averages for comparison\n",
    "overall_means = processed_df[feature_columns + ['survived']].mean()\n",
    "\n",
    "print(f\"\\nOverall dataset averages (for comparison):\")\n",
    "for feature, avg_val in overall_means.items():\n",
    "    if feature in ['pclass', 'sex_encoded', 'is_alone']:\n",
    "        print(f\"‚Ä¢ {feature}: {avg_val:.2f}\")\n",
    "    elif feature == 'age':\n",
    "        print(f\"‚Ä¢ {feature}: {avg_val:.1f} years\")  \n",
    "    elif feature == 'fare':\n",
    "        print(f\"‚Ä¢ {feature}: ${avg_val:.2f}\")\n",
    "    else:\n",
    "        print(f\"‚Ä¢ {feature}: {avg_val:.2f}\")\n",
    "\n",
    "print(f\"\\nCluster deviations from average:\")\n",
    "for cluster_id in sorted(cluster_distribution.index):\n",
    "    print(f\"\\n--- CLUSTER {cluster_id} ---\")\n",
    "    cluster_data = cluster_means.loc[cluster_id]\n",
    "    \n",
    "    notable_features = []\n",
    "    \n",
    "    # Compare each feature to overall average\n",
    "    for feature in feature_columns:\n",
    "        cluster_val = cluster_data[feature]\n",
    "        overall_val = overall_means[feature]\n",
    "        \n",
    "        # Calculate relative difference\n",
    "        if overall_val != 0:\n",
    "            rel_diff = (cluster_val - overall_val) / overall_val * 100\n",
    "            \n",
    "            if abs(rel_diff) > 20:  # Only show significant differences\n",
    "                direction = \"higher\" if rel_diff"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
