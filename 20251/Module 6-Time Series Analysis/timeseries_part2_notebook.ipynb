{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Pandas & Practical Data Manipulation\n",
    "\n",
    "## Time Series Analysis in Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Before we begin, install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Time Series Data Structures in Pandas\n",
    "\n",
    "## Overview\n",
    "\n",
    "Pandas provides several specialized data structures for working with time series:\n",
    "\n",
    "1. **Timestamp**: A single point in time (like `datetime`)\n",
    "2. **DatetimeIndex**: An array of Timestamps (index for time series)\n",
    "3. **Period**: A fixed-frequency interval (e.g., January 2023)\n",
    "4. **PeriodIndex**: An array of Periods\n",
    "5. **Timedelta**: Duration of time\n",
    "6. **TimedeltaIndex**: An array of Timedeltas\n",
    "\n",
    "## Why Use DatetimeIndex?\n",
    "\n",
    "- Efficient time-based indexing and slicing\n",
    "- Automatic date arithmetic\n",
    "- Built-in resampling and frequency conversion\n",
    "- Integration with matplotlib for time-based plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creating Timestamps and DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single timestamp\n",
    "timestamp = pd.Timestamp('2023-01-15')\n",
    "print(\"Single Timestamp:\")\n",
    "print(timestamp)\n",
    "print(f\"Type: {type(timestamp)}\")\n",
    "\n",
    "# Creating DatetimeIndex from list\n",
    "dates_list = ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "dt_index = pd.DatetimeIndex(dates_list)\n",
    "print(\"\\nDatetimeIndex from list:\")\n",
    "print(dt_index)\n",
    "\n",
    "# Using pd.to_datetime\n",
    "dates_str = pd.Series(['2023/01/01', '2023/01/02', '2023/01/03'])\n",
    "dates_converted = pd.to_datetime(dates_str)\n",
    "print(\"\\nConverted with to_datetime:\")\n",
    "print(dates_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Using pd.date_range()\n",
    "\n",
    "The most common way to create a DatetimeIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily frequency\n",
    "daily = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')\n",
    "print(\"Daily dates:\")\n",
    "print(daily)\n",
    "\n",
    "# Using periods instead of end date\n",
    "weekly = pd.date_range(start='2023-01-01', periods=8, freq='W')\n",
    "print(\"\\nWeekly dates (8 periods):\")\n",
    "print(weekly)\n",
    "\n",
    "# Business days only\n",
    "business_days = pd.date_range(start='2023-01-01', periods=10, freq='B')\n",
    "print(\"\\nBusiness days:\")\n",
    "print(business_days)\n",
    "\n",
    "# Hourly frequency\n",
    "hourly = pd.date_range(start='2023-01-01', periods=24, freq='H')\n",
    "print(\"\\nHourly (first 5):\")\n",
    "print(hourly[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Frequency Strings\n",
    "\n",
    "| Code | Description | Code | Description |\n",
    "|------|-------------|------|-------------|\n",
    "| D | Calendar day | H | Hourly |\n",
    "| B | Business day | T or min | Minute |\n",
    "| W | Weekly | S | Second |\n",
    "| M | Month end | MS | Month start |\n",
    "| Q | Quarter end | QS | Quarter start |\n",
    "| Y | Year end | YS | Year start |\n",
    "| BH | Business hour | L | Millisecond |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Period and PeriodIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single period\n",
    "period = pd.Period('2023-01', freq='M')\n",
    "print(\"Single Period (month):\")\n",
    "print(period)\n",
    "print(f\"Start: {period.start_time}\")\n",
    "print(f\"End: {period.end_time}\")\n",
    "\n",
    "# PeriodIndex - useful for quarterly/monthly data\n",
    "periods = pd.period_range(start='2023-01', end='2023-12', freq='M')\n",
    "print(\"\\nMonthly periods for 2023:\")\n",
    "print(periods)\n",
    "\n",
    "# Quarterly periods\n",
    "quarters = pd.period_range(start='2023Q1', end='2024Q4', freq='Q')\n",
    "print(\"\\nQuarterly periods:\")\n",
    "print(quarters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Time Zones\n",
    "\n",
    "Pandas supports timezone-aware datetime objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timezone-naive dates\n",
    "dates_naive = pd.date_range('2023-01-01', periods=5, freq='D')\n",
    "print(\"Timezone-naive:\")\n",
    "print(dates_naive)\n",
    "print(f\"Timezone: {dates_naive.tz}\")\n",
    "\n",
    "# Localize to a timezone\n",
    "dates_utc = dates_naive.tz_localize('UTC')\n",
    "print(\"\\nLocalized to UTC:\")\n",
    "print(dates_utc)\n",
    "\n",
    "# Convert to different timezone\n",
    "dates_ny = dates_utc.tz_convert('America/New_York')\n",
    "print(\"\\nConverted to New York:\")\n",
    "print(dates_ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data Loading & Preparation\n",
    "\n",
    "## 2.1 Creating Time Series DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', periods=365, freq='D')\n",
    "data = {\n",
    "    'sales': np.random.randint(100, 500, size=365),\n",
    "    'customers': np.random.randint(10, 100, size=365),\n",
    "    'temperature': 15 + 10 * np.sin(2 * np.pi * np.arange(365) / 365) + np.random.normal(0, 2, 365)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "df['revenue'] = df['sales'] * np.random.uniform(10, 20, size=365)\n",
    "\n",
    "print(\"Time Series DataFrame:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Index type: {type(df.index)}\")\n",
    "print(f\"\\nIndex info:\")\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loading Time Series from CSV\n",
    "\n",
    "Common patterns for reading time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample data to CSV first\n",
    "df.to_csv('sample_timeseries.csv')\n",
    "\n",
    "# Method 1: Parse dates during read\n",
    "df_loaded = pd.read_csv('sample_timeseries.csv', \n",
    "                        index_col=0,\n",
    "                        parse_dates=True)\n",
    "print(\"Loaded from CSV:\")\n",
    "print(df_loaded.head())\n",
    "print(f\"Index type: {type(df_loaded.index)}\")\n",
    "\n",
    "# Method 2: Set index after loading\n",
    "df_alt = pd.read_csv('sample_timeseries.csv')\n",
    "df_alt['date'] = pd.to_datetime(df_alt.iloc[:, 0])\n",
    "df_alt = df_alt.set_index('date')\n",
    "df_alt = df_alt.drop(df_alt.columns[0], axis=1)\n",
    "\n",
    "print(\"\\nAlternative method:\")\n",
    "print(df_alt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Handling Missing Data\n",
    "\n",
    "Time series often have missing values that need special treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "df_missing = df.copy()\n",
    "df_missing.loc['2023-01-05':'2023-01-07', 'sales'] = np.nan\n",
    "df_missing.loc['2023-01-15', 'customers'] = np.nan\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(df_missing.loc['2023-01-01':'2023-01-10'])\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values count:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Forward fill - use last valid observation\n",
    "df_ffill = df_missing.fillna(method='ffill')\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_ffill.loc['2023-01-01':'2023-01-10', 'sales'])\n",
    "\n",
    "# Backward fill\n",
    "df_bfill = df_missing.fillna(method='bfill')\n",
    "print(\"\\nBackward fill:\")\n",
    "print(df_bfill.loc['2023-01-01':'2023-01-10', 'sales'])\n",
    "\n",
    "# Interpolation - linear by default\n",
    "df_interp = df_missing.interpolate(method='linear')\n",
    "print(\"\\nLinear interpolation:\")\n",
    "print(df_interp.loc['2023-01-01':'2023-01-10', 'sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Methods\n",
    "\n",
    "| Method | Description | Best For |\n",
    "|--------|-------------|----------|\n",
    "| linear | Linear interpolation | Default, general purpose |\n",
    "| time | Time-weighted interpolation | Irregularly spaced time series |\n",
    "| polynomial | Polynomial interpolation | Smooth curves |\n",
    "| spline | Spline interpolation | Smooth curves with control |\n",
    "| nearest | Nearest neighbor | Categorical-like data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different interpolation methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "methods = ['linear', 'polynomial', 'spline', 'nearest']\n",
    "for ax, method in zip(axes.flatten(), methods):\n",
    "    if method == 'polynomial':\n",
    "        interpolated = df_missing['sales'].interpolate(method=method, order=2)\n",
    "    elif method == 'spline':\n",
    "        interpolated = df_missing['sales'].interpolate(method=method, order=2)\n",
    "    else:\n",
    "        interpolated = df_missing['sales'].interpolate(method=method)\n",
    "    \n",
    "    df_missing['sales'].plot(ax=ax, style='o', label='Original (with NaN)', alpha=0.5)\n",
    "    interpolated.plot(ax=ax, label=f'{method.capitalize()} interpolation', linewidth=2)\n",
    "    ax.set_title(f'{method.capitalize()} Interpolation', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.set_xlim('2023-01-01', '2023-01-31')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Time-Based Indexing and Slicing\n",
    "\n",
    "## 3.1 Basic Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single date\n",
    "print(\"Data for 2023-01-15:\")\n",
    "print(df.loc['2023-01-15'])\n",
    "\n",
    "# Date range\n",
    "print(\"\\nData from Jan 1 to Jan 5:\")\n",
    "print(df.loc['2023-01-01':'2023-01-05'])\n",
    "\n",
    "# Partial string indexing - very powerful!\n",
    "print(\"\\nAll data for January 2023:\")\n",
    "print(df.loc['2023-01'].head())\n",
    "print(f\"Shape: {df.loc['2023-01'].shape}\")\n",
    "\n",
    "# All data for Q1\n",
    "print(\"\\nFirst quarter (Jan-Mar):\")\n",
    "print(df.loc['2023-01':'2023-03'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Advanced Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .between_time() for intraday data\n",
    "# First create hourly data\n",
    "hourly_dates = pd.date_range('2023-01-01', periods=24*7, freq='H')\n",
    "hourly_data = pd.DataFrame({\n",
    "    'value': np.random.randn(24*7)\n",
    "}, index=hourly_dates)\n",
    "\n",
    "# Select data between specific times\n",
    "business_hours = hourly_data.between_time('09:00', '17:00')\n",
    "print(\"Business hours (9am-5pm):\")\n",
    "print(business_hours.head(10))\n",
    "\n",
    "# Using .at_time() for specific time\n",
    "noon_data = hourly_data.at_time('12:00')\n",
    "print(\"\\nData at noon each day:\")\n",
    "print(noon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DatetimeIndex Attributes\n",
    "\n",
    "Extract useful information from the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features\n",
    "df['year'] = df.index.year\n",
    "df['month'] = df.index.month\n",
    "df['day'] = df.index.day\n",
    "df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "df['day_name'] = df.index.day_name()\n",
    "df['week_of_year'] = df.index.isocalendar().week\n",
    "df['quarter'] = df.index.quarter\n",
    "df['is_weekend'] = df.index.dayofweek >= 5\n",
    "\n",
    "print(\"DataFrame with temporal features:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Useful for grouping and analysis\n",
    "print(\"\\nAverage sales by day of week:\")\n",
    "print(df.groupby('day_name')['sales'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Resampling and Frequency Conversion\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Resampling** is the process of converting time series from one frequency to another:\n",
    "\n",
    "- **Downsampling**: Higher to lower frequency (daily → monthly)\n",
    "  - Requires aggregation (mean, sum, etc.)\n",
    "  \n",
    "- **Upsampling**: Lower to higher frequency (monthly → daily)\n",
    "  - Requires filling missing values\n",
    "\n",
    "## 4.1 Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean dataframe for resampling\n",
    "df_clean = df[['sales', 'customers', 'revenue']].copy()\n",
    "\n",
    "# Resample to weekly (sum)\n",
    "weekly_sum = df_clean.resample('W').sum()\n",
    "print(\"Weekly totals:\")\n",
    "print(weekly_sum.head())\n",
    "\n",
    "# Resample to monthly (mean)\n",
    "monthly_mean = df_clean.resample('M').mean()\n",
    "print(\"\\nMonthly averages:\")\n",
    "print(monthly_mean.head())\n",
    "\n",
    "# Resample to monthly (multiple aggregations)\n",
    "monthly_agg = df_clean.resample('M').agg({\n",
    "    'sales': ['sum', 'mean', 'std'],\n",
    "    'customers': ['sum', 'mean'],\n",
    "    'revenue': 'sum'\n",
    "})\n",
    "print(\"\\nMonthly aggregations:\")\n",
    "print(monthly_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Common Aggregation Functions\n",
    "\n",
    "| Function | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| sum() | Sum of values | Sales, revenue |\n",
    "| mean() | Average | Temperature, prices |\n",
    "| median() | Middle value | Robust average |\n",
    "| std() | Standard deviation | Volatility |\n",
    "| min() / max() | Extremes | Temperature ranges |\n",
    "| first() / last() | First/last value | Opening/closing prices |\n",
    "| count() | Number of observations | Data availability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize daily vs weekly data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Daily data\n",
    "df_clean['sales'].plot(ax=axes[0], alpha=0.7, label='Daily')\n",
    "axes[0].set_title('Daily Sales', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sales', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly data\n",
    "weekly_sum['sales'].plot(ax=axes[1], marker='o', linewidth=2, label='Weekly (sum)', color='orange')\n",
    "axes[1].set_title('Weekly Sales (Resampled)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Sales', fontsize=11)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with monthly data\n",
    "monthly_data = df_clean.resample('M').sum()\n",
    "print(\"Monthly data:\")\n",
    "print(monthly_data.head())\n",
    "\n",
    "# Upsample to daily and forward fill\n",
    "daily_ffill = monthly_data.resample('D').ffill()\n",
    "print(\"\\nUpsampled to daily (forward fill):\")\n",
    "print(daily_ffill.head(35))  # Show Jan + Feb\n",
    "\n",
    "# Upsample and interpolate\n",
    "daily_interp = monthly_data.resample('D').interpolate(method='linear')\n",
    "print(\"\\nUpsampled to daily (interpolated):\")\n",
    "print(daily_interp.head(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Using .asfreq() for Frequency Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asfreq() vs resample()\n",
    "# asfreq: changes frequency but doesn't aggregate\n",
    "# resample: changes frequency AND aggregates\n",
    "\n",
    "# Select every 7th day (weekly sampling)\n",
    "weekly_asfreq = df_clean.asfreq('W')\n",
    "print(\"Using asfreq('W'):\")\n",
    "print(weekly_asfreq.head())\n",
    "\n",
    "# Compare with resample\n",
    "weekly_resample = df_clean.resample('W').mean()\n",
    "print(\"\\nUsing resample('W').mean():\")\n",
    "print(weekly_resample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Rolling Window Operations\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Rolling windows** compute statistics over a moving window of data. Essential for:\n",
    "- Smoothing noisy data\n",
    "- Identifying trends\n",
    "- Creating features for ML models\n",
    "- Technical analysis in finance\n",
    "\n",
    "## 5.1 Basic Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-day rolling mean\n",
    "df_clean['sales_ma7'] = df_clean['sales'].rolling(window=7).mean()\n",
    "\n",
    "# 30-day rolling mean\n",
    "df_clean['sales_ma30'] = df_clean['sales'].rolling(window=30).mean()\n",
    "\n",
    "# Rolling standard deviation (volatility)\n",
    "df_clean['sales_std7'] = df_clean['sales'].rolling(window=7).std()\n",
    "\n",
    "# Rolling min and max\n",
    "df_clean['sales_min7'] = df_clean['sales'].rolling(window=7).min()\n",
    "df_clean['sales_max7'] = df_clean['sales'].rolling(window=7).max()\n",
    "\n",
    "print(\"DataFrame with rolling features:\")\n",
    "print(df_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Original vs smoothed\n",
    "df_clean['sales'].plot(ax=axes[0], alpha=0.3, label='Original', color='gray')\n",
    "df_clean['sales_ma7'].plot(ax=axes[0], label='7-day MA', linewidth=2)\n",
    "df_clean['sales_ma30'].plot(ax=axes[0], label='30-day MA', linewidth=2)\n",
    "axes[0].set_title('Sales with Moving Averages', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sales', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling standard deviation (volatility)\n",
    "df_clean['sales_std7'].plot(ax=axes[1], color='red', linewidth=2)\n",
    "axes[1].set_title('7-day Rolling Standard Deviation (Volatility)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Std Dev', fontsize=11)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Custom Rolling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function: range (max - min)\n",
    "def rolling_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "df_clean['sales_range7'] = df_clean['sales'].rolling(window=7).apply(rolling_range)\n",
    "\n",
    "# Custom function: coefficient of variation\n",
    "def coef_variation(x):\n",
    "    return x.std() / x.mean() if x.mean() != 0 else 0\n",
    "\n",
    "df_clean['sales_cv7'] = df_clean['sales'].rolling(window=7).apply(coef_variation)\n",
    "\n",
    "print(\"Custom rolling statistics:\")\n",
    "print(df_clean[['sales', 'sales_range7', 'sales_cv7']].dropna().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Expanding Windows\n",
    "\n",
    "Unlike rolling windows (fixed size), expanding windows include all data from the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding mean - cumulative average\n",
    "df_clean['sales_expanding_mean'] = df_clean['sales'].expanding().mean()\n",
    "\n",
    "# Expanding sum - cumulative sum\n",
    "df_clean['sales_cumsum'] = df_clean['sales'].expanding().sum()\n",
    "\n",
    "# Or use cumsum() directly\n",
    "df_clean['sales_cumsum2'] = df_clean['sales'].cumsum()\n",
    "\n",
    "print(\"Expanding window statistics:\")\n",
    "print(df_clean[['sales', 'sales_expanding_mean', 'sales_cumsum']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize expanding mean vs rolling mean\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "df_clean['sales'].plot(ax=ax, alpha=0.3, label='Original', color='gray')\n",
    "df_clean['sales_ma30'].plot(ax=ax, label='30-day Rolling Mean', linewidth=2)\n",
    "df_clean['sales_expanding_mean'].plot(ax=ax, label='Expanding Mean', linewidth=2)\n",
    "\n",
    "ax.set_title('Comparison: Rolling vs Expanding Mean', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Sales', fontsize=11)\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Exponentially Weighted Functions\n",
    "\n",
    "Give more weight to recent observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentially weighted moving average (EWMA)\n",
    "df_clean['sales_ewm_short'] = df_clean['sales'].ewm(span=7, adjust=False).mean()\n",
    "df_clean['sales_ewm_long'] = df_clean['sales'].ewm(span=30, adjust=False).mean()\n",
    "\n",
    "# Exponentially weighted standard deviation\n",
    "df_clean['sales_ewm_std'] = df_clean['sales'].ewm(span=7, adjust=False).std()\n",
    "\n",
    "print(\"Exponentially weighted statistics:\")\n",
    "print(df_clean[['sales', 'sales_ewm_short', 'sales_ewm_long']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare simple MA vs EWMA\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "df_clean['sales'].plot(ax=ax, alpha=0.3, label='Original', color='gray', linewidth=1)\n",
    "df_clean['sales_ma7'].plot(ax=ax, label='7-day Simple MA', linewidth=2)\n",
    "df_clean['sales_ewm_short'].plot(ax=ax, label='7-day EWMA', linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_title('Simple Moving Average vs Exponential Moving Average', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Sales', fontsize=11)\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim('2023-01-01', '2023-03-31')  # Focus on Q1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Lag and Shift Operations\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Lagging** creates features from previous time steps - crucial for:\n",
    "- Creating autoregressive features\n",
    "- Computing changes and growth rates\n",
    "- Building ML models\n",
    "\n",
    "## 6.1 Basic Shift Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean dataframe for lag operations\n",
    "df_lag = df[['sales', 'customers']].copy()\n",
    "\n",
    "# Lag 1 (previous day)\n",
    "df_lag['sales_lag1'] = df_lag['sales'].shift(1)\n",
    "\n",
    "# Lag 7 (same day last week)\n",
    "df_lag['sales_lag7'] = df_lag['sales'].shift(7)\n",
    "\n",
    "# Lead 1 (next day) - negative shift\n",
    "df_lag['sales_lead1'] = df_lag['sales'].shift(-1)\n",
    "\n",
    "print(\"Lag and lead features:\")\n",
    "print(df_lag.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Computing Changes and Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference from previous period\n",
    "df_lag['sales_diff1'] = df_lag['sales'].diff(1)\n",
    "\n",
    "# Percentage change\n",
    "df_lag['sales_pct_change'] = df_lag['sales'].pct_change(1) * 100\n",
    "\n",
    "# Week-over-week change\n",
    "df_lag['sales_diff7'] = df_lag['sales'].diff(7)\n",
    "df_lag['sales_pct_change7'] = df_lag['sales'].pct_change(7) * 100\n",
    "\n",
    "print(\"Changes and growth rates:\")\n",
    "print(df_lag[['sales', 'sales_diff1', 'sales_pct_change', 'sales_pct_change7']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs changes\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Original series\n",
    "df_lag['sales'].plot(ax=axes[0], linewidth=1.5, color='steelblue')\n",
    "axes[0].set_title('Original Sales', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sales', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# First difference\n",
    "df_lag['sales_diff1'].plot(ax=axes[1], linewidth=1, color='orange')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].set_title('Day-over-Day Change', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Change', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Percentage change\n",
    "df_lag['sales_pct_change'].plot(ax=axes[2], linewidth=1, color='green')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[2].set_title('Percentage Change', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('% Change', fontsize=11)\n",
    "axes[2].set_xlabel('Date', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Creating Multiple Lags for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple lag features efficiently\n",
    "def create_lags(df, column, lags):\n",
    "    \"\"\"\n",
    "    Create multiple lag features for a column\n",
    "    \"\"\"\n",
    "    for lag in lags:\n",
    "        df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Create lags 1-7 and 14, 21, 28 (weekly patterns)\n",
    "df_ml = df[['sales']].copy()\n",
    "lags = list(range(1, 8)) + [14, 21, 28]\n",
    "df_ml = create_lags(df_ml, 'sales', lags)\n",
    "\n",
    "print(\"ML-ready dataframe with lags:\")\n",
    "print(df_ml.head(30))\n",
    "print(f\"\\nShape: {df_ml.shape}\")\n",
    "print(f\"Columns: {df_ml.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Simple Forecasting Methods\n",
    "\n",
    "## Theory\n",
    "\n",
    "Simple methods serve as baselines before applying complex models:\n",
    "\n",
    "1. **Naive**: Last observed value\n",
    "2. **Seasonal Naive**: Same period from last cycle\n",
    "3. **Moving Average**: Average of recent observations\n",
    "4. **Exponential Smoothing**: Weighted average giving more weight to recent data\n",
    "\n",
    "## 7.1 Naive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train-test split\n",
    "df_forecast = df[['sales']].copy()\n",
    "train_size = int(len(df_forecast) * 0.8)\n",
    "train = df_forecast[:train_size]\n",
    "test = df_forecast[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Test size: {len(test)}\")\n",
    "\n",
    "# Naive forecast: use last value\n",
    "naive_forecast = pd.Series(\n",
    "    train['sales'].iloc[-1], \n",
    "    index=test.index,\n",
    "    name='naive'\n",
    ")\n",
    "\n",
    "# Seasonal naive: use same day from last week\n",
    "seasonal_naive_forecast = pd.Series(\n",
    "    train['sales'].iloc[-7:].values,\n",
    "    index=test.index[:7],\n",
    "    name='seasonal_naive'\n",
    ")\n",
    "# Repeat pattern for rest of test period\n",
    "for i in range(7, len(test), 7):\n",
    "    end_idx = min(i+7, len(test))\n",
    "    seasonal_naive_forecast = pd.concat([\n",
    "        seasonal_naive_forecast,\n",
    "        pd.Series(train['sales'].iloc[-7:-(7-end_idx+i)].values if end_idx < i+7 else train['sales'].iloc[-7:].values,\n",
    "                 index=test.index[i:end_idx])\n",
    "    ])\n",
    "\n",
    "print(\"\\nNaive forecast (first 10 days):\")\n",
    "print(naive_forecast.head(10))\n",
    "print(\"\\nSeasonal naive forecast (first 10 days):\")\n",
    "print(seasonal_naive_forecast.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Moving Average Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple moving average forecast\n",
    "window = 7\n",
    "ma_forecast = pd.Series(\n",
    "    train['sales'].iloc[-window:].mean(),\n",
    "    index=test.index,\n",
    "    name='moving_average'\n",
    ")\n",
    "\n",
    "print(\"Moving average forecast (first 10 days):\")\n",
    "print(ma_forecast.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Exponential Smoothing\n",
    "\n",
    "### Single Exponential Smoothing (SES)\n",
    "\n",
    "**Formula**: ŷₜ = α·yₜ₋₁ + (1-α)·ŷₜ₋₁\n",
    "\n",
    "Where α (alpha) is the smoothing parameter (0 < α < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "\n",
    "# Single Exponential Smoothing\n",
    "ses_model = SimpleExpSmoothing(train['sales'])\n",
    "ses_fit = ses_model.fit(smoothing_level=0.2, optimized=False)\n",
    "ses_forecast = ses_fit.forecast(steps=len(test))\n",
    "\n",
    "print(\"Single Exponential Smoothing forecast:\")\n",
    "print(ses_forecast.head(10))\n",
    "print(f\"\\nSmoothing level (alpha): {ses_fit.params['smoothing_level']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt's Linear Trend (Double Exponential Smoothing)\n",
    "\n",
    "Extends SES to capture trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt's method (with trend)\n",
    "holt_model = ExponentialSmoothing(\n",
    "    train['sales'], \n",
    "    trend='add',\n",
    "    seasonal=None\n",
    ")\n",
    "holt_fit = holt_model.fit()\n",
    "holt_forecast = holt_fit.forecast(steps=len(test))\n",
    "\n",
    "print(\"Holt's Linear Trend forecast:\")\n",
    "print(holt_forecast.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt-Winters (Triple Exponential Smoothing)\n",
    "\n",
    "Captures level, trend, and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt-Winters method (with trend and seasonality)\n",
    "hw_model = ExponentialSmoothing(\n",
    "    train['sales'],\n",
    "    trend='add',\n",
    "    seasonal='add',\n",
    "    seasonal_periods=7  # weekly seasonality\n",
    ")\n",
    "hw_fit = hw_model.fit()\n",
    "hw_forecast = hw_fit.forecast(steps=len(test))\n",
    "\n",
    "print(\"Holt-Winters forecast:\")\n",
    "print(hw_forecast.head(10))\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"Smoothing level (alpha): {hw_fit.params['smoothing_level']:.4f}\")\n",
    "print(f\"Smoothing trend (beta): {hw_fit.params['smoothing_trend']:.4f}\")\n",
    "print(f\"Smoothing seasonal (gamma): {hw_fit.params['smoothing_seasonal']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Comparing Forecast Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all forecasts\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot training data\n",
    "train['sales'].plot(ax=ax, label='Train', linewidth=1.5, color='black')\n",
    "\n",
    "# Plot test data\n",
    "test['sales'].plot(ax=ax, label='Test (Actual)', linewidth=2, color='blue')\n",
    "\n",
    "# Plot forecasts\n",
    "naive_forecast.plot(ax=ax, label='Naive', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "ma_forecast.plot(ax=ax, label='Moving Avg', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "ses_forecast.plot(ax=ax, label='SES', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "holt_forecast.plot(ax=ax, label='Holt', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "hw_forecast.plot(ax=ax, label='Holt-Winters', linestyle='--', linewidth=2, alpha=0.9)\n",
    "\n",
    "ax.set_title('Comparison of Simple Forecasting Methods', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Sales', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Evaluating Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def evaluate_forecast(actual, forecast, method_name):\n",
    "    \"\"\"\n",
    "    Calculate MAE, RMSE, MAPE for a forecast\n",
    "    \"\"\"\n",
    "    # Align indices\n",
    "    common_idx = actual.index.intersection(forecast.index)\n",
    "    actual_aligned = actual.loc[common_idx]\n",
    "    forecast_aligned = forecast.loc[common_idx]\n",
    "    \n",
    "    mae = mean_absolute_error(actual_aligned, forecast_aligned)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_aligned, forecast_aligned))\n",
    "    mape = np.mean(np.abs((actual_aligned - forecast_aligned) / actual_aligned)) * 100\n",
    "    \n",
    "    return {\n",
    "        'Method': method_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Evaluate all methods\n",
    "results = []\n",
    "results.append(evaluate_forecast(test['sales'], naive_forecast, 'Naive'))\n",
    "results.append(evaluate_forecast(test['sales'], ma_forecast, 'Moving Average'))\n",
    "results.append(evaluate_forecast(test['sales'], ses_forecast, 'SES'))\n",
    "results.append(evaluate_forecast(test['sales'], holt_forecast, 'Holt'))\n",
    "results.append(evaluate_forecast(test['sales'], hw_forecast, 'Holt-Winters'))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\nForecast Evaluation Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Best method: {results_df.iloc[0]['Method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## Key Pandas Time Series Operations\n",
    "\n",
    "### 1. Data Structures\n",
    "- Use `DatetimeIndex` for time series data\n",
    "- Create with `pd.date_range()` or `pd.to_datetime()`\n",
    "- Consider `Period` for interval data (months, quarters)\n",
    "\n",
    "### 2. Data Preparation\n",
    "- Handle missing values with `ffill()`, `bfill()`, or `interpolate()`\n",
    "- Always set datetime as index for time series operations\n",
    "- Extract temporal features: day, month, day_of_week, etc.\n",
    "\n",
    "### 3. Resampling\n",
    "- **Downsampling**: Use `.resample('W').sum()` or `.mean()`\n",
    "- **Upsampling**: Use `.resample('D').ffill()` or `.interpolate()`\n",
    "- Choose appropriate aggregation function\n",
    "\n",
    "### 4. Rolling Windows\n",
    "- **Fixed windows**: `.rolling(window=7).mean()`\n",
    "- **Expanding windows**: `.expanding().mean()`\n",
    "- **Exponential**: `.ewm(span=7).mean()`\n",
    "- Use for smoothing and creating features\n",
    "\n",
    "### 5. Lag Operations\n",
    "- Create lags: `.shift(1)` for previous period\n",
    "- Calculate changes: `.diff()` and `.pct_change()`\n",
    "- Essential for ML feature engineering\n",
    "\n",
    "### 6. Simple Forecasting\n",
    "- Start with naive methods as baseline\n",
    "- Moving averages for simple smoothing\n",
    "- Exponential smoothing (SES, Holt, Holt-Winters) for trend/seasonality\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Always visualize** your data and transformations\n",
    "2. **Check for missing values** before analysis\n",
    "3. **Choose appropriate frequency** for your data\n",
    "4. **Consider seasonality** when selecting window sizes\n",
    "5. **Compare multiple methods** before choosing one\n",
    "6. **Use proper train-test splits** (chronological)\n",
    "\n",
    "## Common Pandas Time Series Functions\n",
    "\n",
    "```python\n",
    "# Date ranges\n",
    "pd.date_range(start, end, freq)\n",
    "pd.period_range(start, end, freq)\n",
    "\n",
    "# Resampling\n",
    "df.resample('W').sum()\n",
    "df.asfreq('D')\n",
    "\n",
    "# Rolling\n",
    "df.rolling(window=7).mean()\n",
    "df.expanding().mean()\n",
    "df.ewm(span=7).mean()\n",
    "\n",
    "# Shifting\n",
    "df.shift(1)\n",
    "df.diff()\n",
    "df.pct_change()\n",
    "\n",
    "# Time-based selection\n",
    "df.loc['2023-01']\n",
    "df.between_time('09:00', '17:00')\n",
    "df.at_time('12:00')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Part 3**, we'll cover:\n",
    "- Feature engineering for machine learning models\n",
    "- Traditional ML approaches (Random Forest, XGBoost, LightGBM)\n",
    "- Deep learning for time series (LSTM, GRU, 1D CNN)\n",
    "- Advanced tools (Prophet, NeuralProphet)\n",
    "- Model evaluation and selection strategies\n",
    "- Production deployment considerations\n",
    "\n",
    "---\n",
    "\n",
    "*End of Part 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}